apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel-daemonset
  namespace: observability
spec:
  mode: daemonset

  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
  
  config:
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133

    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${env:MY_POD_IP}:4317
          http:
            endpoint: ${env:MY_POD_IP}:4318

      filelog:
        include:
          - /var/log/pods/*/*/*.log
        exclude:
          # Exclude logs from all containers named otel-collector
          - /var/log/pods/*/opentelemetry-collector/*.log
        start_at: end
        include_file_path: true
        include_file_name: false
        operators:
          - id: container-parser
            max_log_size: 102400
            type: container
        retry_on_failure:
          enabled: true

    processors:
      batch:
        send_batch_max_size: 1500
        send_batch_size: 1000
        timeout: 1s

      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25

      k8sattributes:
        extract:
          metadata: # extracted from the pod
            - k8s.namespace.name
            - k8s.deployment.name
            - k8s.statefulset.name
            - k8s.daemonset.name
            - k8s.cronjob.name
            - k8s.job.name
            - k8s.node.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.pod.start_time
        filter:
          node_from_env_var: K8S_NODE_NAME
        passthrough: false
        pod_association: # How to associate the data to a pod (order matters)
          - sources: # First try to use the value of the resource attribute k8s.pod.ip
              - from: resource_attribute
                name: k8s.pod.ip
          - sources: # Then try to use the value of the resource attribute k8s.pod.uid
              - from: resource_attribute
                name: k8s.pod.uid
          - sources: # If neither of those work, use the request's connection to get the pod IP.
              - from: connection
      resource:
        attributes:
          - key: cluster
            value: "production"
            action: upsert

    exporters:
      debug: {}
      otlphttp/logs:
        endpoint: "http://loki-distributor.observability.svc.cluster.local:3100/otlp"
        tls:
          insecure: true
      otlphttp/metrics:
        endpoint: "http://prometheus-kube-prometheus-prometheus.observability.svc.cluster.local:9090/api/v1/otlp"
        tls:
          insecure: true
      otlp/traces:
        endpoint: "http://jaeger-collector.observability.svc.cluster.local:4317"
        tls:
          insecure: true

    service:
      extensions:
        - health_check
      pipelines:
        logs:
          exporters:
            - debug
            # - otlphttp/logs
          processors:
            - k8sattributes
            - memory_limiter
            - batch
          receivers:
            - otlp
            - filelog
        metrics:
          exporters:
            - otlphttp/metrics
          processors:
            - k8sattributes
            - memory_limiter
            - batch
          receivers:
            - otlp
        traces:
          exporters:
            # - otlp/traces
            - debug
          processors:
            - k8sattributes
            - memory_limiter
            - batch
          receivers:
            - otlp

      telemetry:
        metrics:
          address: ${env:MY_POD_IP}:8888
  
  volumeMounts:
    - name: varlogpods
      mountPath: /var/log/pods
      readOnly: true
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true

  volumes:
    - name: varlogpods
      hostPath:
        path: /var/log/pods
        type: ''
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers
        type: ''